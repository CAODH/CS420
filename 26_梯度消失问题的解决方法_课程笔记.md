### 梯度消失问题的解决方法

1. 梯度消失问题
   - 使用Sigmoid激活函数，梯度范围可能会变得越来越小
   - 在反向传播超过5层之后，梯度可能会消失

2. 采用ReLU函数环节梯度消失问题
   - x增加时ReLU的梯度不会消失
3. 深度残差网络
   - 有时，即时在训练数据上更深层的网络性能也可能比较浅层的网络差
   -  ResNet的构造块
   - 残差网络在ImageNet上的表现
4. 批标准化
   - 在神经网络的训练中输入分布会发生变化
   - 在输入之前做批标准化，对批量数据做标准化，减去平均值除标准差，标准化之后分布的平均值为0方差为1
   - 批标准化实验