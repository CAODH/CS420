### 激活函数与损失函数

1. 激活函数
   1. Sigmoid激活函数
      - 导数，输出范围[0,1]
      - 受生物神经元启发产生，可以被看做一个人工神经元在当前输入下被“激活”的概率
      - 边界值会使Sigmoid函数的梯度消失，导致底层神经网络层梯度更新缓慢
   2. Tanh激活函数
      - 导数，输出范围[-1,1]
      - 负值较大的输入经过tanh函数映射为负输出，只有接近零的输入会被映射为接近零的输出
      - 使网络训练时被“卡主”的可能性降低
   3. 线性整流函数（Rectified Linear Unit，ReLU）
      - 导数
      - x增加时ReLU的梯度不会消失
      - 可以用来对正值输入进行建模
      - 计算速度很快，因为无需计算指数函数
      - 可以不再需要“预训练”过程
      - 允许稀疏表示

2. 误差/损失函数

   - 随机梯度下降：从随机抽取的样本中进行更新（实际上执行批处理更新）

     - $\theta_{new} = \theta_{old} -\eta\frac{\partial L(\theta)}{\partial \theta}$  

   - 多分类问题的Softmax损失（交叉熵损失）

     

